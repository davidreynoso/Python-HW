{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Pandas DataFrames\n",
    "You need to be able to do/ know the following:\n",
    "- Access a column of a DataFrame.\n",
    "- Create a new Dataframe from an existing one consisting of a subset of columns.\n",
    "- Create a new Dataframe from an existing one consisting of specific rows with specific indices.\n",
    "- Create a new Dataframe from an existing one consisting of all rows satisfying a certain property.\n",
    "- Use groupby to create summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Access a column of a DataFrame:\n",
    "You can access a column of a DataFrame by using square brackets with the column name as the key. df[\"column_name\"]\n",
    "\n",
    "### 2.Create a new DataFrame from an existing one consisting of a subset of columns:\n",
    "To create a new DataFrame with only a subset of columns from an existing DataFrame, you can use the square brackets with a list of the columns you want to include. new_df = df[[\"column_1\", \"column_2\"]]\n",
    "\n",
    "### 3.Create a new DataFrame from an existing one consisting of specific rows with specific indices:\n",
    "To create a new DataFrame with specific rows from an existing DataFrame, you can use the loc method with a list of the row indices you want to include. For example, if you have a DataFrame called df with row indices 0, 1, 2, and 3 and you only want to include rows 1 and 3, you can do: new_df = df.loc[[1, 3]]\n",
    "\n",
    "### 4.Create a new DataFrame from an existing one consisting of all rows satisfying a certain property:\n",
    "To create a new DataFrame with all rows that satisfy a certain condition, you can use boolean indexing. For example, if you have a DataFrame called df with a column called \"column_name\" and you want to include only the rows where \"column_name\" is greater than 5, you can do: new_df = df[df[\"column_name\"] > 5]\n",
    "\n",
    "### 5.Use groupby to create summaries:\n",
    "To create summaries using groupby, you can group the DataFrame by one or more columns and then apply a summary function to each group. For example, if you have a DataFrame called df with columns \"column_1\" and \"column_2\" and you want to compute the mean of \"column_2\" for each value of \"column_1\", you can do: \n",
    "df.groupby(\"column_1\")[\"column_2\"].mean()\n",
    "This will group the DataFrame by the \"column_1\" column and compute the mean of the \"column_2\" column for each group. The result will be a Series with the mean value for each unique value of \"column_1\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>acquisitionYear</th>\n",
       "      <th>gender</th>\n",
       "      <th>dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Figure Bowing before a Seated Old Man with h...</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two Drawings of Frightened Figures, Probably f...</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Preaching of Warning. Verso: An Old Man En...</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Six Drawings of Figures with Outstretched Arms</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Circle of the Lustful: Francesca da Rimini...</td>\n",
       "      <td>1919.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>2D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  acquisitionYear gender  \\\n",
       "0  A Figure Bowing before a Seated Old Man with h...           1922.0   Male   \n",
       "1  Two Drawings of Frightened Figures, Probably f...           1922.0   Male   \n",
       "2  The Preaching of Warning. Verso: An Old Man En...           1922.0   Male   \n",
       "3     Six Drawings of Figures with Outstretched Arms           1922.0   Male   \n",
       "4  The Circle of the Lustful: Francesca da Rimini...           1919.0   Male   \n",
       "\n",
       "  dimension  \n",
       "0        2D  \n",
       "1        2D  \n",
       "2        2D  \n",
       "3        2D  \n",
       "4        2D  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artwork = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-12/artwork.csv')\n",
    "artists = pd.read_csv(\"https://github.com/tategallery/collection/raw/master/artist_data.csv\")\n",
    "\n",
    "artwork.to_csv(\"artwork.csv\", index = False)\n",
    "artists.to_csv(\"artists.csv\", index = False)\n",
    "artwork = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-12/artwork.csv')\n",
    "artists = pd.read_csv(\"https://github.com/tategallery/collection/raw/master/artist_data.csv\")\n",
    "\n",
    "artwork[\"id\"] = artwork[\"artistId\"]\n",
    "artwork = artwork[[\"id\", \"year\", \"acquisitionYear\", \"title\", \"medium\"]]\n",
    "artists = artists[[\"id\", \"gender\"]]\n",
    "df = pd.merge(artwork, artists)\n",
    "\n",
    "def dimension(med_string):\n",
    "    \"\"\"\n",
    "    Assign a dimension to a given piece of artwork based on the description\n",
    "    of the medium, supplied as a string. \n",
    "    Media that include the words \"paper\", \"canvas\", \"oil\", or \"paint\" are assumed \n",
    "    2D. \n",
    "    Media that are not 2d and include the words \"bronze\", \"stone\", or \"ceramic\" are \n",
    "    assumed 3D. \n",
    "    Otherwise, the media is \"Other/Unknown\"\n",
    "    \n",
    "    @param med_string: str, the original medium\n",
    "    @return dim: one of \"2D\", \"3D\", or \"Other/Unknown\" according to the rules above. \n",
    "    \"\"\"\n",
    "    if type(med_string) != str:\n",
    "        med_string = str(med_string)\n",
    "    med_string = med_string.lower()\n",
    "    if any([w in med_string for w in [\"paper\", \"canvas\", \"oil\", \"paint\"]]):\n",
    "        return \"2D\"\n",
    "    elif any([w in med_string for w in [\"bronze\", \"stone\", \"ceramic\"]]):\n",
    "        return \"3D\"\n",
    "    else:\n",
    "        return \"Other/Unknown\"\n",
    "\n",
    "df[\"dimension\"] = [dimension(m) for m in df[\"medium\"]]\n",
    "df = df[[\"title\",\"acquisitionYear\", \"gender\", \"dimension\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the series for the dimension column\n",
    "dimension_series = df[\"dimension\"]\n",
    "dimension_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a DataFrame consisting of all rows of acquisitionYear and gender\n",
    "acquisition_gender_df = df[[\"acquisitionYear\", \"gender\"]]\n",
    "acquisition_gender_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a DataFrame consisting of all the rows of acquisitionYear and gender\n",
    "# for which the dimension is 2D\n",
    "acquisition_gender_2d_df = df.loc[df[\"dimension\"] == \"2D\", [\"acquisitionYear\", \"gender\"]]\n",
    "acquisition_gender_2d_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupby to compute the number of male and female paintings over the last 10 years\n",
    "recent_df = df.loc[df[\"acquisitionYear\"] >= 2011]  # select rows for the last 10 years\n",
    "gender_counts = recent_df.groupby(\"gender\").size()  # group by gender and count the number of rows in each group\n",
    "gender_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace bad values with NaN\n",
    "df = df.replace(\"Unknown\", np.nan)  # replace \"Unknown\" values with NaN\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "- When should we split into a train and test set? What are some reasonable ratios which people use in practice?\n",
    "\n",
    "- What is the purpose of the training set vs the test set?\n",
    "\n",
    "- For the Penguins data set, or any dataset for that matter, using dropna() straight away is a poor idea for which reasons?\n",
    "\n",
    "- Provide some valid reasons for removing a feature or column from a dataset prior to performing any form of feature selection.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. When should we split into a train and test set? What are some reasonable ratios which people use in practice?\n",
    "We should split into a train and test set after we have preprocessed and cleaned the data. This is done in order to evaluate the performance of a machine learning model on unseen data. A common ratio used in practice is 80:20, where 80% of the data is used for training and 20% for testing. However, the ratio may vary depending on the size and complexity of the dataset.\n",
    "\n",
    "\n",
    "### 2. What is the purpose of the training set vs the test set?\n",
    "The purpose of the training set is to train a machine learning model on a portion of the data. The model learns the patterns and relationships in the data and adjusts its parameters accordingly. The purpose of the test set is to evaluate the performance of the trained model on unseen data. This helps us to determine how well the model generalizes to new data.\n",
    "\n",
    "\n",
    "### 3. For the Penguins data set, or any dataset for that matter, using dropna() straight away is a poor idea for which reasons?\n",
    "Using dropna() straight away can be a poor idea because it can result in the loss of important information. If there are only a few missing values, it may be better to impute the missing values using methods such as mean imputation or interpolation. Additionally, dropping too many rows with missing values can result in a biased dataset and affect the performance of a machine learning model.\n",
    "\n",
    "\n",
    "### 4. Provide some valid reasons for removing a feature or column from a dataset prior to performing any form of feature selection.\n",
    "The feature has little or no impact on the target variable.\n",
    "The feature is redundant or highly correlated with another feature.\n",
    "The feature contains a lot of missing values and cannot be imputed.\n",
    "The feature is noisy or contains outliers that cannot be handled.\n",
    "The feature is not relevant or meaningful to the problem being solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>studyName</th>\n",
       "      <th>Sample Number</th>\n",
       "      <th>Species</th>\n",
       "      <th>Region</th>\n",
       "      <th>Island</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Individual ID</th>\n",
       "      <th>Clutch Completion</th>\n",
       "      <th>Date Egg</th>\n",
       "      <th>Culmen Length (mm)</th>\n",
       "      <th>Culmen Depth (mm)</th>\n",
       "      <th>Flipper Length (mm)</th>\n",
       "      <th>Body Mass (g)</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Delta 15 N (o/oo)</th>\n",
       "      <th>Delta 13 C (o/oo)</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PAL0708</td>\n",
       "      <td>1</td>\n",
       "      <td>Adelie Penguin (Pygoscelis adeliae)</td>\n",
       "      <td>Anvers</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>Adult, 1 Egg Stage</td>\n",
       "      <td>N1A1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11/11/07</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not enough blood for isotopes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PAL0708</td>\n",
       "      <td>2</td>\n",
       "      <td>Adelie Penguin (Pygoscelis adeliae)</td>\n",
       "      <td>Anvers</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>Adult, 1 Egg Stage</td>\n",
       "      <td>N1A2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11/11/07</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>8.94956</td>\n",
       "      <td>-24.69454</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAL0708</td>\n",
       "      <td>3</td>\n",
       "      <td>Adelie Penguin (Pygoscelis adeliae)</td>\n",
       "      <td>Anvers</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>Adult, 1 Egg Stage</td>\n",
       "      <td>N2A1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11/16/07</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>8.36821</td>\n",
       "      <td>-25.33302</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PAL0708</td>\n",
       "      <td>4</td>\n",
       "      <td>Adelie Penguin (Pygoscelis adeliae)</td>\n",
       "      <td>Anvers</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>Adult, 1 Egg Stage</td>\n",
       "      <td>N2A2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11/16/07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adult not sampled.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAL0708</td>\n",
       "      <td>5</td>\n",
       "      <td>Adelie Penguin (Pygoscelis adeliae)</td>\n",
       "      <td>Anvers</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>Adult, 1 Egg Stage</td>\n",
       "      <td>N3A1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11/16/07</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>8.76651</td>\n",
       "      <td>-25.32426</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  studyName  Sample Number                              Species  Region  \\\n",
       "0   PAL0708              1  Adelie Penguin (Pygoscelis adeliae)  Anvers   \n",
       "1   PAL0708              2  Adelie Penguin (Pygoscelis adeliae)  Anvers   \n",
       "2   PAL0708              3  Adelie Penguin (Pygoscelis adeliae)  Anvers   \n",
       "3   PAL0708              4  Adelie Penguin (Pygoscelis adeliae)  Anvers   \n",
       "4   PAL0708              5  Adelie Penguin (Pygoscelis adeliae)  Anvers   \n",
       "\n",
       "      Island               Stage Individual ID Clutch Completion  Date Egg  \\\n",
       "0  Torgersen  Adult, 1 Egg Stage          N1A1               Yes  11/11/07   \n",
       "1  Torgersen  Adult, 1 Egg Stage          N1A2               Yes  11/11/07   \n",
       "2  Torgersen  Adult, 1 Egg Stage          N2A1               Yes  11/16/07   \n",
       "3  Torgersen  Adult, 1 Egg Stage          N2A2               Yes  11/16/07   \n",
       "4  Torgersen  Adult, 1 Egg Stage          N3A1               Yes  11/16/07   \n",
       "\n",
       "   Culmen Length (mm)  Culmen Depth (mm)  Flipper Length (mm)  Body Mass (g)  \\\n",
       "0                39.1               18.7                181.0         3750.0   \n",
       "1                39.5               17.4                186.0         3800.0   \n",
       "2                40.3               18.0                195.0         3250.0   \n",
       "3                 NaN                NaN                  NaN            NaN   \n",
       "4                36.7               19.3                193.0         3450.0   \n",
       "\n",
       "      Sex  Delta 15 N (o/oo)  Delta 13 C (o/oo)  \\\n",
       "0    MALE                NaN                NaN   \n",
       "1  FEMALE            8.94956          -24.69454   \n",
       "2  FEMALE            8.36821          -25.33302   \n",
       "3     NaN                NaN                NaN   \n",
       "4  FEMALE            8.76651          -25.32426   \n",
       "\n",
       "                         Comments  \n",
       "0  Not enough blood for isotopes.  \n",
       "1                             NaN  \n",
       "2                             NaN  \n",
       "3              Adult not sampled.  \n",
       "4                             NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://philchodrow.github.io/PIC16A/datasets/palmer_penguins.csv\"\n",
    "penguins = pd.read_csv(url)\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many NaN values/ rows are there per feature / column?\n",
    "penguins.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which columns can be disregarded almost immediatly and why?\n",
    "penguins = penguins.drop([\"studyName\", \"year\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data, the \"studyName\" column can be disregarded almost immediately since it only has one unique value, as well as the \"year\" column since it is likely not relevant to our analysis. We can drop them using the drop() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pg_train, pg_test = train_test_split(penguins, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupby to create a table with rows island by gender\n",
    "# and columns body mass and flipper length which displays the\n",
    "# median and mean\n",
    "penguins.groupby([\"island\", \"sex\"])[[\"body_mass_g\", \"flipper_length_mm\"]].agg([\"median\", \"mean\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add species information by color and gender or island information as dot type\n",
    "# Which of these pairs of features do you think has good potential to discriminate \n",
    "# between penguin species and why?\n",
    "sns.pairplot(data = pg_train,x=\"bill_depth_mm\", y=\"body_mass_g\", hue=\"species\", style=\"sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a model\n",
    "- What does fitting a model typically entail? Use a decision tree or polynomial as an example and explain what fitting means in this context.\n",
    "\n",
    "- What is the difference between clustering, regression and classification? In particular what are the outputs of the these models and what do we give them as inputs?\n",
    "\n",
    "- What is the difference between supervised and unsupervised learning? Give some examples covered in class.\n",
    "\n",
    "- In your own words describe what a model, target variable and predictor variable are in the context of supervised learning. Give an example we have covered in class.\n",
    "\n",
    "- When someone talks about the expressivity or complexity of a model family, what are they referring to. Explain this concept using as an example decision trees.\n",
    "\n",
    "- The class of depth 5 decision trees is more expressive or complex than the class of depth 3 decision trees, true or false? Explain why.\n",
    "\n",
    "- What is validation data used for when fitting a model? Is it a subset of the training or test data?\n",
    "\n",
    "- Describe what happens in k-fold cross validation. Why do we use k-fold cross validation as opposed to setting aside a fixed part of the training set for validation?\n",
    "\n",
    "- What is overfitting? How would you recognize it? What steps would you take to solve overfitting? Use polynomial regression as an example\n",
    "\n",
    "- What is underfitting? How would you recognize it? What steps would you take to solve underfitting? Use a decision tree as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What does fitting a model typically entail? Use a decision tree or polynomial as an example and explain what fitting means in this context.\n",
    "Fitting a model typically entails using an algorithm to learn the relationship between the input variables (predictors) and the output variable (target) in a dataset. For example, fitting a decision tree model involves finding the optimal splitting points in the data to create a tree structure that can predict the target variable. Fitting a polynomial model involves finding the coefficients of a polynomial function that can approximate the relationship between the predictors and the target variable.\n",
    "\n",
    "### 2. What is the difference between clustering, regression and classification? In particular what are the outputs of the these models and what do we give them as inputs?\n",
    "Clustering, regression, and classification are all types of machine learning models, but they differ in their goals and the types of outputs they produce. Clustering algorithms group similar data points together based on some measure of similarity, with no specific target variable. Regression models predict a continuous target variable based on input predictors, while classification models predict a categorical target variable based on input predictors.\n",
    "\n",
    "### 3. What is the difference between supervised and unsupervised learning? Give some examples covered in class.\n",
    "Supervised learning involves training a model on labeled data where the target variable is known, while unsupervised learning involves training a model on unlabeled data where the target variable is unknown. Examples of supervised learning include linear regression and logistic regression, while examples of unsupervised learning include k-means clustering and principal component analysis.\n",
    "\n",
    "### 4. In your own words describe what a model, target variable and predictor variable are in the context of supervised learning. Give an example we have covered in class.\n",
    "In supervised learning, a model is trained to predict a target variable based on input predictors. The target variable is the variable that we want to predict, and the predictor variables are the variables we use to make the prediction. For example, in the case of predicting penguin body mass based on flipper length and species, the target variable would be body mass, and the predictor variables would be flipper length and species.\n",
    "\n",
    "### 5. When someone talks about the expressivity or complexity of a model family, what are they referring to. Explain this concept using as an example decision trees.\n",
    "When someone talks about the expressivity or complexity of a model family, they are referring to the ability of the model to represent complex relationships between the input predictors and the target variable. For example, decision trees with a greater depth have a higher level of expressivity because they can represent more complex decision boundaries.\n",
    "\n",
    "### 6. The class of depth 5 decision trees is more expressive or complex than the class of depth 3 decision trees, true or false? Explain why.\n",
    "True. A depth 5 decision tree is more expressive than a depth 3 decision tree because it can represent more complex decision boundaries. This is because a depth 5 tree can represent more complex functions with more splits and branches than a depth 3 tree, which can only represent simpler functions with fewer splits and branches.\n",
    "\n",
    "### 7. What is validation data used for when fitting a model? Is it a subset of the training or test data?\n",
    "Validation data is used to evaluate the performance of a model during training and prevent overfitting. It is typically a subset of the training data that is not used during training, but is used to evaluate the performance of the model on data it has not seen before. Validation data is used for evaluating the performance of a model during training. It is typically a subset of the training data that is held out and not used for training, but instead used for evaluating the model's performance on unseen data. This helps to prevent overfitting to the training data and ensures that the model is able to generalize well to new data.\n",
    "\n",
    "### 8. Describe what happens in k-fold cross validation. Why do we use k-fold cross validation as opposed to setting aside a fixed part of the training set for validation?\n",
    "In k-fold cross validation, the data is divided into k equally sized partitions, and the model is trained k times, each time using a different partition as the validation set and the remaining data as the training set. The performance of the model is then evaluated as the average performance across the k folds. We use k-fold cross validation instead of setting aside a fixed part of the training set for validation because it allows us to evaluate the performance of the model on different parts of the data and reduces the variance in the estimated performance. In k-fold cross validation, the training data is split into k subsets of roughly equal size. The model is trained on k-1 subsets and evaluated on the remaining subset. This process is repeated k times, each time with a different subset held out for evaluation. The final performance of the model is then averaged across all k folds. This approach helps to reduce the variance in the performance estimate and provides a more reliable estimate of the model's performance.\n",
    "\n",
    "### 9. What is overfitting? How would you recognize it? What steps would you take to solve overfitting? Use polynomial regression as an example\n",
    "Overfitting occurs when a model is too complex and fits the training data too closely, leading to poor performance on new, unseen data. Overfitting can be recognized when the model has a high training accuracy but a low test accuracy. To solve overfitting, one can use techniques such as regularization, early stopping, or reducing the complexity of the model. In polynomial regression, one can reduce the degree of the polynomial or use regularization techniques such as L1 or L2 regularization. Overfitting occurs when a model is too complex and fits the training data too closely, resulting in poor performance on new, unseen data. Overfitting can be recognized by a large difference between the performance on the training data and the performance on the validation or test data. To solve overfitting in polynomial regression, one could reduce the degree of the polynomial or add regularization to the model.\n",
    "\n",
    "### 10. What is underfitting? How would you recognize it? What steps would you take to solve underfitting? Use a decision tree as an example.\n",
    "Underfitting occurs when a model is too simple and fails to capture the underlying relationship between the predictors and the target variable, leading to poor performance on both the training and test data. Underfitting can be recognized when the model has a low training accuracy and a low test accuracy. To solve underfitting, one can use techniques such as increasing the complexity of the model, increasing the number of features, or using a more powerful algorithm. To solve underfitting in decision trees, one could increase the depth of the tree or add more features to the dataset. Adding more features to a decision tree can improve its performance in several ways. First, it increases the amount of information available for the tree to make decisions. More features provide the tree with more potential splits, which can increase the accuracy of the model.\n",
    "\n",
    "Second, adding more features can help the decision tree to capture more complex relationships between the input and output variables. The tree can use these additional features to create more nuanced splits, which may better capture the underlying patterns in the data.\n",
    "\n",
    "However, it is important to note that adding too many features can also have a negative impact on the performance of the decision tree. This is because some features may be redundant or irrelevant, and including them can lead to overfitting. Therefore, it is important to carefully select which features to include in the model, and to use techniques such as feature selection or regularization to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Owen Harris Braund</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. John Bradley (Florence Briggs Thayer) Cum...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss. Laina Heikkinen</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. Jacques Heath (Lily May Peel) Futrelle</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. William Henry Allen</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass                                               Name  Sex  \\\n",
       "0         0       3                             Mr. Owen Harris Braund    1   \n",
       "1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...    0   \n",
       "2         1       3                              Miss. Laina Heikkinen    0   \n",
       "3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle    0   \n",
       "4         0       3                            Mr. William Henry Allen    1   \n",
       "\n",
       "    Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
       "0  22.0                        1                        0   7.2500  \n",
       "1  38.0                        1                        0  71.2833  \n",
       "2  26.0                        0                        0   7.9250  \n",
       "3  35.0                        1                        0  53.1000  \n",
       "4  35.0                        0                        0   8.0500  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://philchodrow.github.io/PIC16A/datasets/titanic.csv\"\n",
    "titanic = pd.read_csv(url)\n",
    "le = preprocessing.LabelEncoder()\n",
    "titanic['Sex'] = le.fit_transform(titanic['Sex'])\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(titanic[[\"Pclass\", \"Age\", \"Sex\", \"Survived\"]], test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[[\"Pclass\", \"Age\", \"Sex\"]]\n",
    "y_train = train[\"Survived\"]\n",
    "X_test = test[[\"Pclass\", \"Age\", \"Sex\"]]\n",
    "y_test = test[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a tree of depth 2 to the data to predict survivability based on \n",
    "# sex, age and Pclass and outputs its training and test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a tree of depth 8 to the data to predict survivability based on \n",
    "# sex, age and Pclass and outputs its training and test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a tree of depth100 to the data to predict survivability based on \n",
    "# sex, age and Pclass and outputs its training and test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which of the above depths do you think is most appropriate? Discuss\n",
    "# which max_depth parameter you think lead to fitting vs underfitting vs overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use cross validation to train an SVC to predict passenger class using \n",
    "# passenger age, sex and fare. Ue cross validation to find a good choice of gamma.\n",
    "# In particular, plot a graph of cv score vs gamma with a logarithmic x axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic bias\n",
    "\n",
    "- Define in your own words sample bias, population (or historical) bias and measurement bias.\n",
    "\n",
    "- Label the following as examples of the three types of bias above.\n",
    "    - predict likelihood to win an oscar based on first and second name\n",
    "    - predict social media usage in the US by collecting data from students at UCLA\n",
    "    - estimate propensity to score touchdowns based on maximum squat and bench press\n",
    "    \n",
    "- Why is algorithmic bias hard to diagnose?\n",
    "    \n",
    "- How can algorithmic bias result in feedback loops which increase inequity?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
